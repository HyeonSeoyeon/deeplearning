{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4330632e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4271c2a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>15.26</th>\n",
       "      <th>14.84</th>\n",
       "      <th>0.871</th>\n",
       "      <th>5.763</th>\n",
       "      <th>3.312</th>\n",
       "      <th>2.221</th>\n",
       "      <th>5.22</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.88</td>\n",
       "      <td>14.57</td>\n",
       "      <td>0.8811</td>\n",
       "      <td>5.554</td>\n",
       "      <td>3.333</td>\n",
       "      <td>1.018</td>\n",
       "      <td>4.956</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14.29</td>\n",
       "      <td>14.09</td>\n",
       "      <td>0.9050</td>\n",
       "      <td>5.291</td>\n",
       "      <td>3.337</td>\n",
       "      <td>2.699</td>\n",
       "      <td>4.825</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.84</td>\n",
       "      <td>13.94</td>\n",
       "      <td>0.8955</td>\n",
       "      <td>5.324</td>\n",
       "      <td>3.379</td>\n",
       "      <td>2.259</td>\n",
       "      <td>4.805</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.14</td>\n",
       "      <td>14.99</td>\n",
       "      <td>0.9034</td>\n",
       "      <td>5.658</td>\n",
       "      <td>3.562</td>\n",
       "      <td>1.355</td>\n",
       "      <td>5.175</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14.38</td>\n",
       "      <td>14.21</td>\n",
       "      <td>0.8951</td>\n",
       "      <td>5.386</td>\n",
       "      <td>3.312</td>\n",
       "      <td>2.462</td>\n",
       "      <td>4.956</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>12.19</td>\n",
       "      <td>13.20</td>\n",
       "      <td>0.8783</td>\n",
       "      <td>5.137</td>\n",
       "      <td>2.981</td>\n",
       "      <td>3.631</td>\n",
       "      <td>4.870</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>11.23</td>\n",
       "      <td>12.88</td>\n",
       "      <td>0.8511</td>\n",
       "      <td>5.140</td>\n",
       "      <td>2.795</td>\n",
       "      <td>4.325</td>\n",
       "      <td>5.003</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>13.20</td>\n",
       "      <td>13.66</td>\n",
       "      <td>0.8883</td>\n",
       "      <td>5.236</td>\n",
       "      <td>3.232</td>\n",
       "      <td>8.315</td>\n",
       "      <td>5.056</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>11.84</td>\n",
       "      <td>13.21</td>\n",
       "      <td>0.8521</td>\n",
       "      <td>5.175</td>\n",
       "      <td>2.836</td>\n",
       "      <td>3.598</td>\n",
       "      <td>5.044</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>12.30</td>\n",
       "      <td>13.34</td>\n",
       "      <td>0.8684</td>\n",
       "      <td>5.243</td>\n",
       "      <td>2.974</td>\n",
       "      <td>5.637</td>\n",
       "      <td>5.063</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>209 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     15.26  14.84   0.871  5.763  3.312  2.221   5.22  1\n",
       "0    14.88  14.57  0.8811  5.554  3.333  1.018  4.956  1\n",
       "1    14.29  14.09  0.9050  5.291  3.337  2.699  4.825  1\n",
       "2    13.84  13.94  0.8955  5.324  3.379  2.259  4.805  1\n",
       "3    16.14  14.99  0.9034  5.658  3.562  1.355  5.175  1\n",
       "4    14.38  14.21  0.8951  5.386  3.312  2.462  4.956  1\n",
       "..     ...    ...     ...    ...    ...    ...    ... ..\n",
       "204  12.19  13.20  0.8783  5.137  2.981  3.631  4.870  3\n",
       "205  11.23  12.88  0.8511  5.140  2.795  4.325  5.003  3\n",
       "206  13.20  13.66  0.8883  5.236  3.232  8.315  5.056  3\n",
       "207  11.84  13.21  0.8521  5.175  2.836  3.598  5.044  3\n",
       "208  12.30  13.34  0.8684  5.243  2.974  5.637  5.063  3\n",
       "\n",
       "[209 rows x 8 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 로드\n",
    "df = pd.read_csv(\"D:/seeds.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "54156b33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "결측치 확인:\n",
      "15.26    0\n",
      "14.84    0\n",
      "0.871    0\n",
      "5.763    0\n",
      "3.312    0\n",
      "2.221    0\n",
      "5.22     0\n",
      "1        0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 결측치 확인\n",
    "print(\"\\n결측치 확인:\")\n",
    "print(df.isnull().sum()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "39d756cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "imbalanced data 확인:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 레이블 개수 확인(imbalanced data) - 그래프\n",
    "print(\"\\nimbalanced data 확인:\")\n",
    "plt.figure(figsize=(6, 4))  # 그래프 크기 설정\n",
    "\n",
    "sns.countplot(data = df, x=\"1\")\n",
    "plt.xlabel(\"1\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8b53686c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[14.88  , 14.57  ,  0.8811, ...,  3.333 ,  1.018 ,  4.956 ],\n",
       "       [14.29  , 14.09  ,  0.905 , ...,  3.337 ,  2.699 ,  4.825 ],\n",
       "       [13.84  , 13.94  ,  0.8955, ...,  3.379 ,  2.259 ,  4.805 ],\n",
       "       ...,\n",
       "       [13.2   , 13.66  ,  0.8883, ...,  3.232 ,  8.315 ,  5.056 ],\n",
       "       [11.84  , 13.21  ,  0.8521, ...,  2.836 ,  3.598 ,  5.044 ],\n",
       "       [12.3   , 13.34  ,  0.8684, ...,  2.974 ,  5.637 ,  5.063 ]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 입력값과 타겟 분리\n",
    "X = df.iloc[:, :-1].values\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0e3a95eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], dtype=int64)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 입력값과 타겟 분리\n",
    "y = df.iloc[:, -1].values -1  # 레이블 0,1,2로 변환\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6f502390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "레이블 개수 확인:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2    70\n",
       "3    70\n",
       "1    69\n",
       "Name: 1, dtype: int64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 레이블 확인 - 숫자\n",
    "print(\"\\n레이블 개수 확인:\")\n",
    "df['1'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a63e033a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.0118402 ,  0.00923364,  0.42749407, ...,  0.19764747,\n",
       "        -1.79278662, -0.9219711 ],\n",
       "       [-0.19093968, -0.35835326,  1.43894519, ...,  0.20823799,\n",
       "        -0.67216102, -1.18860657],\n",
       "       [-0.3456023 , -0.47322416,  1.03690395, ...,  0.31943844,\n",
       "        -0.9654836 , -1.22931428],\n",
       "       ...,\n",
       "       [-0.56556692, -0.68764985,  0.73219901, ..., -0.06976315,\n",
       "         3.07170181, -0.71843257],\n",
       "       [-1.03299173, -1.03226257, -0.79978973, ..., -1.11822457,\n",
       "        -0.07284964, -0.7428572 ],\n",
       "       [-0.87489216, -0.93270779, -0.10997159, ..., -0.75285165,\n",
       "         1.28643389, -0.70418488]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 정규화\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9ce6340c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 원-핫 인코딩\n",
    "y = to_categorical(y)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6271c77e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((167, 7), (42, 7), (167, 3), (42, 3))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 학습/테스트 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9812cde7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tjdus\\anaconda3\\lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 117ms/step - accuracy: 0.3697 - loss: 1.2227 - val_accuracy: 0.2647 - val_loss: 1.3110\n",
      "Epoch 2/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.3701 - loss: 1.1565 - val_accuracy: 0.2647 - val_loss: 1.2281\n",
      "Epoch 3/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.3818 - loss: 1.0927 - val_accuracy: 0.2647 - val_loss: 1.1523\n",
      "Epoch 4/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.3571 - loss: 1.0488 - val_accuracy: 0.2647 - val_loss: 1.0859\n",
      "Epoch 5/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.3942 - loss: 0.9910 - val_accuracy: 0.2941 - val_loss: 1.0265\n",
      "Epoch 6/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.4513 - loss: 0.9408 - val_accuracy: 0.3235 - val_loss: 0.9736\n",
      "Epoch 7/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.4703 - loss: 0.9147 - val_accuracy: 0.4706 - val_loss: 0.9280\n",
      "Epoch 8/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.5771 - loss: 0.8730 - val_accuracy: 0.5882 - val_loss: 0.8891\n",
      "Epoch 9/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.6554 - loss: 0.8528 - val_accuracy: 0.7059 - val_loss: 0.8563\n",
      "Epoch 10/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.7463 - loss: 0.8330 - val_accuracy: 0.8529 - val_loss: 0.8259\n",
      "Epoch 11/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.8572 - loss: 0.8082 - val_accuracy: 0.8824 - val_loss: 0.7982\n",
      "Epoch 12/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.9122 - loss: 0.8013 - val_accuracy: 0.9118 - val_loss: 0.7740\n",
      "Epoch 13/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.9056 - loss: 0.7819 - val_accuracy: 0.9412 - val_loss: 0.7523\n",
      "Epoch 14/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 0.9130 - loss: 0.7640 - val_accuracy: 1.0000 - val_loss: 0.7318\n",
      "Epoch 15/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.8941 - loss: 0.7519 - val_accuracy: 1.0000 - val_loss: 0.7116\n",
      "Epoch 16/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.9010 - loss: 0.7367 - val_accuracy: 1.0000 - val_loss: 0.6915\n",
      "Epoch 17/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.9012 - loss: 0.7024 - val_accuracy: 1.0000 - val_loss: 0.6717\n",
      "Epoch 18/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.9069 - loss: 0.6855 - val_accuracy: 1.0000 - val_loss: 0.6551\n",
      "Epoch 19/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.8887 - loss: 0.6848 - val_accuracy: 1.0000 - val_loss: 0.6403\n",
      "Epoch 20/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.8774 - loss: 0.6737 - val_accuracy: 1.0000 - val_loss: 0.6269\n",
      "Epoch 21/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.8770 - loss: 0.6610 - val_accuracy: 1.0000 - val_loss: 0.6141\n",
      "Epoch 22/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.8757 - loss: 0.6508 - val_accuracy: 1.0000 - val_loss: 0.6013\n",
      "Epoch 23/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.8826 - loss: 0.6383 - val_accuracy: 1.0000 - val_loss: 0.5889\n",
      "Epoch 24/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9026 - loss: 0.6153 - val_accuracy: 1.0000 - val_loss: 0.5767\n",
      "Epoch 25/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.8796 - loss: 0.6145 - val_accuracy: 1.0000 - val_loss: 0.5658\n",
      "Epoch 26/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.8991 - loss: 0.5946 - val_accuracy: 1.0000 - val_loss: 0.5547\n",
      "Epoch 27/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.8704 - loss: 0.5978 - val_accuracy: 1.0000 - val_loss: 0.5457\n",
      "Epoch 28/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.8747 - loss: 0.5855 - val_accuracy: 1.0000 - val_loss: 0.5360\n",
      "Epoch 29/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8898 - loss: 0.5719 - val_accuracy: 1.0000 - val_loss: 0.5265\n",
      "Epoch 30/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.8842 - loss: 0.5651 - val_accuracy: 1.0000 - val_loss: 0.5161\n",
      "Epoch 31/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.8963 - loss: 0.5460 - val_accuracy: 1.0000 - val_loss: 0.5069\n",
      "Epoch 32/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.8975 - loss: 0.5512 - val_accuracy: 1.0000 - val_loss: 0.5005\n",
      "Epoch 33/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9148 - loss: 0.5346 - val_accuracy: 1.0000 - val_loss: 0.4925\n",
      "Epoch 34/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.9118 - loss: 0.5257 - val_accuracy: 1.0000 - val_loss: 0.4840\n",
      "Epoch 35/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9036 - loss: 0.5307 - val_accuracy: 1.0000 - val_loss: 0.4745\n",
      "Epoch 36/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8932 - loss: 0.5156 - val_accuracy: 1.0000 - val_loss: 0.4644\n",
      "Epoch 37/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.8846 - loss: 0.4980 - val_accuracy: 1.0000 - val_loss: 0.4558\n",
      "Epoch 38/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 0.9141 - loss: 0.4835 - val_accuracy: 1.0000 - val_loss: 0.4461\n",
      "Epoch 39/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.8941 - loss: 0.4934 - val_accuracy: 1.0000 - val_loss: 0.4392\n",
      "Epoch 40/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9184 - loss: 0.4672 - val_accuracy: 1.0000 - val_loss: 0.4327\n",
      "Epoch 41/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9068 - loss: 0.4613 - val_accuracy: 1.0000 - val_loss: 0.4268\n",
      "Epoch 42/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9114 - loss: 0.4745 - val_accuracy: 1.0000 - val_loss: 0.4203\n",
      "Epoch 43/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9045 - loss: 0.4707 - val_accuracy: 1.0000 - val_loss: 0.4133\n",
      "Epoch 44/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8993 - loss: 0.4696 - val_accuracy: 1.0000 - val_loss: 0.4083\n",
      "Epoch 45/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.8996 - loss: 0.4549 - val_accuracy: 1.0000 - val_loss: 0.4030\n",
      "Epoch 46/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.8970 - loss: 0.4456 - val_accuracy: 1.0000 - val_loss: 0.3967\n",
      "Epoch 47/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.9187 - loss: 0.4314 - val_accuracy: 1.0000 - val_loss: 0.3895\n",
      "Epoch 48/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.9093 - loss: 0.4231 - val_accuracy: 1.0000 - val_loss: 0.3813\n",
      "Epoch 49/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9323 - loss: 0.4146 - val_accuracy: 1.0000 - val_loss: 0.3746\n",
      "Epoch 50/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8945 - loss: 0.4363 - val_accuracy: 1.0000 - val_loss: 0.3674\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step \n",
      "Sequential Model Accuracy: 0.9285714285714286\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.90      0.86        10\n",
      "           1       1.00      1.00      1.00        15\n",
      "           2       0.94      0.88      0.91        17\n",
      "\n",
      "    accuracy                           0.93        42\n",
      "   macro avg       0.92      0.93      0.92        42\n",
      "weighted avg       0.93      0.93      0.93        42\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 9  0  1]\n",
      " [ 0 15  0]\n",
      " [ 2  0 15]]\n"
     ]
    }
   ],
   "source": [
    "# Model 1: Sequential 모델 (add()로 레이어 추가)\n",
    "\n",
    "# Sequential 모델 정의\n",
    "model1 = Sequential()\n",
    "model1.add(Dense(units=32, input_shape=(X.shape[1],), activation='sigmoid'))\n",
    "model1.add(Dense(units=3, activation='softmax'))\n",
    "\n",
    "model1.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model1.fit(X_train, y_train, epochs=50, validation_split=0.2)\n",
    "\n",
    "# 예측 및 평가\n",
    "y_pred_prob = model1.predict(X_test)\n",
    "y_pred_class = np.argmax(y_pred_prob, axis=1)\n",
    "y_test_class = np.argmax(y_test, axis=1)\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(\"Sequential Model Accuracy:\", np.mean(y_test_class == y_pred_class))\n",
    "print(classification_report(y_test_class, y_pred_class))\n",
    "print(confusion_matrix(y_test_class, y_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e1988a46",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.3158 - loss: 1.2855 - val_accuracy: 0.3824 - val_loss: 1.2388\n",
      "Epoch 2/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.3492 - loss: 1.1995 - val_accuracy: 0.3824 - val_loss: 1.1797\n",
      "Epoch 3/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.3313 - loss: 1.1428 - val_accuracy: 0.3824 - val_loss: 1.1303\n",
      "Epoch 4/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.3865 - loss: 1.1006 - val_accuracy: 0.4412 - val_loss: 1.0843\n",
      "Epoch 5/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.5166 - loss: 1.0269 - val_accuracy: 0.5294 - val_loss: 1.0411\n",
      "Epoch 6/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.5646 - loss: 1.0146 - val_accuracy: 0.5882 - val_loss: 0.9996\n",
      "Epoch 7/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.6460 - loss: 0.9826 - val_accuracy: 0.6176 - val_loss: 0.9639\n",
      "Epoch 8/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.6508 - loss: 0.9301 - val_accuracy: 0.6471 - val_loss: 0.9344\n",
      "Epoch 9/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.6478 - loss: 0.9185 - val_accuracy: 0.6471 - val_loss: 0.9042\n",
      "Epoch 10/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.6047 - loss: 0.9182 - val_accuracy: 0.6471 - val_loss: 0.8737\n",
      "Epoch 11/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.6273 - loss: 0.8703 - val_accuracy: 0.6471 - val_loss: 0.8476\n",
      "Epoch 12/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.6341 - loss: 0.8613 - val_accuracy: 0.7059 - val_loss: 0.8248\n",
      "Epoch 13/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.6549 - loss: 0.8297 - val_accuracy: 0.7353 - val_loss: 0.8010\n",
      "Epoch 14/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.7281 - loss: 0.8205 - val_accuracy: 0.8824 - val_loss: 0.7785\n",
      "Epoch 15/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.8290 - loss: 0.8024 - val_accuracy: 0.9706 - val_loss: 0.7560\n",
      "Epoch 16/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9093 - loss: 0.7813 - val_accuracy: 0.9706 - val_loss: 0.7360\n",
      "Epoch 17/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9049 - loss: 0.7706 - val_accuracy: 1.0000 - val_loss: 0.7186\n",
      "Epoch 18/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.8804 - loss: 0.7479 - val_accuracy: 0.9706 - val_loss: 0.7004\n",
      "Epoch 19/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.8847 - loss: 0.7434 - val_accuracy: 0.9706 - val_loss: 0.6836\n",
      "Epoch 20/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 0.8825 - loss: 0.7145 - val_accuracy: 0.9706 - val_loss: 0.6685\n",
      "Epoch 21/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.8960 - loss: 0.7241 - val_accuracy: 0.9706 - val_loss: 0.6550\n",
      "Epoch 22/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8808 - loss: 0.7037 - val_accuracy: 0.9706 - val_loss: 0.6424\n",
      "Epoch 23/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8757 - loss: 0.6834 - val_accuracy: 1.0000 - val_loss: 0.6319\n",
      "Epoch 24/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.9041 - loss: 0.6600 - val_accuracy: 1.0000 - val_loss: 0.6199\n",
      "Epoch 25/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 0.9087 - loss: 0.6525 - val_accuracy: 1.0000 - val_loss: 0.6103\n",
      "Epoch 26/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9166 - loss: 0.6474 - val_accuracy: 1.0000 - val_loss: 0.6011\n",
      "Epoch 27/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9152 - loss: 0.6340 - val_accuracy: 0.9706 - val_loss: 0.5928\n",
      "Epoch 28/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9205 - loss: 0.6198 - val_accuracy: 0.9706 - val_loss: 0.5844\n",
      "Epoch 29/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9123 - loss: 0.6012 - val_accuracy: 0.9706 - val_loss: 0.5719\n",
      "Epoch 30/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8984 - loss: 0.6000 - val_accuracy: 1.0000 - val_loss: 0.5589\n",
      "Epoch 31/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9010 - loss: 0.5980 - val_accuracy: 1.0000 - val_loss: 0.5458\n",
      "Epoch 32/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9010 - loss: 0.5873 - val_accuracy: 1.0000 - val_loss: 0.5341\n",
      "Epoch 33/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9067 - loss: 0.5758 - val_accuracy: 1.0000 - val_loss: 0.5243\n",
      "Epoch 34/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.9106 - loss: 0.5540 - val_accuracy: 1.0000 - val_loss: 0.5165\n",
      "Epoch 35/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.9161 - loss: 0.5615 - val_accuracy: 1.0000 - val_loss: 0.5087\n",
      "Epoch 36/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9154 - loss: 0.5315 - val_accuracy: 1.0000 - val_loss: 0.4996\n",
      "Epoch 37/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9075 - loss: 0.5361 - val_accuracy: 1.0000 - val_loss: 0.4877\n",
      "Epoch 38/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9174 - loss: 0.5250 - val_accuracy: 1.0000 - val_loss: 0.4771\n",
      "Epoch 39/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8998 - loss: 0.5070 - val_accuracy: 0.9706 - val_loss: 0.4657\n",
      "Epoch 40/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9110 - loss: 0.5144 - val_accuracy: 0.9706 - val_loss: 0.4557\n",
      "Epoch 41/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9010 - loss: 0.4972 - val_accuracy: 0.9706 - val_loss: 0.4491\n",
      "Epoch 42/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9001 - loss: 0.4947 - val_accuracy: 0.9706 - val_loss: 0.4425\n",
      "Epoch 43/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8975 - loss: 0.4920 - val_accuracy: 1.0000 - val_loss: 0.4363\n",
      "Epoch 44/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9063 - loss: 0.4860 - val_accuracy: 1.0000 - val_loss: 0.4309\n",
      "Epoch 45/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9127 - loss: 0.4731 - val_accuracy: 1.0000 - val_loss: 0.4251\n",
      "Epoch 46/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.8850 - loss: 0.4921 - val_accuracy: 1.0000 - val_loss: 0.4199\n",
      "Epoch 47/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9171 - loss: 0.4532 - val_accuracy: 1.0000 - val_loss: 0.4159\n",
      "Epoch 48/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8932 - loss: 0.4682 - val_accuracy: 1.0000 - val_loss: 0.4097\n",
      "Epoch 49/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.9040 - loss: 0.4673 - val_accuracy: 1.0000 - val_loss: 0.4031\n",
      "Epoch 50/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.8984 - loss: 0.4559 - val_accuracy: 1.0000 - val_loss: 0.3948\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n",
      "Functional API Model Accuracy: 0.9285714285714286\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.90      0.86        10\n",
      "           1       1.00      1.00      1.00        15\n",
      "           2       0.94      0.88      0.91        17\n",
      "\n",
      "    accuracy                           0.93        42\n",
      "   macro avg       0.92      0.93      0.92        42\n",
      "weighted avg       0.93      0.93      0.93        42\n",
      "\n",
      "[[ 9  0  1]\n",
      " [ 0 15  0]\n",
      " [ 2  0 15]]\n"
     ]
    }
   ],
   "source": [
    "# Model 2: 함수형 API\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "\n",
    "# Functional API 모델 정의\n",
    "inputs = Input(shape=(X.shape[1],))\n",
    "x = Dense(32, activation='sigmoid')(inputs)\n",
    "outputs = Dense(3, activation='softmax')(x)\n",
    "model2 = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "model2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model2.fit(X_train, y_train, epochs=50, validation_split=0.2)\n",
    "\n",
    "# 예측 및 평가\n",
    "y_pred_prob = model2.predict(X_test)\n",
    "y_pred_class = np.argmax(y_pred_prob, axis=1)\n",
    "y_test_class = np.argmax(y_test, axis=1)\n",
    "\n",
    "print(\"Functional API Model Accuracy:\", np.mean(y_test_class == y_pred_class))\n",
    "print(classification_report(y_test_class, y_pred_class))\n",
    "print(confusion_matrix(y_test_class, y_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6153be3c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.3310 - loss: 1.0710 - val_accuracy: 0.3235 - val_loss: 1.0512\n",
      "Epoch 2/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.4572 - loss: 1.0178 - val_accuracy: 0.4706 - val_loss: 1.0144\n",
      "Epoch 3/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.5102 - loss: 0.9972 - val_accuracy: 0.5000 - val_loss: 0.9780\n",
      "Epoch 4/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.6585 - loss: 0.9554 - val_accuracy: 0.5882 - val_loss: 0.9480\n",
      "Epoch 5/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.6875 - loss: 0.9316 - val_accuracy: 0.6765 - val_loss: 0.9176\n",
      "Epoch 6/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.7578 - loss: 0.9050 - val_accuracy: 0.7647 - val_loss: 0.8888\n",
      "Epoch 7/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.7849 - loss: 0.8884 - val_accuracy: 0.8824 - val_loss: 0.8595\n",
      "Epoch 8/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.8408 - loss: 0.8606 - val_accuracy: 0.9706 - val_loss: 0.8329\n",
      "Epoch 9/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.8718 - loss: 0.8396 - val_accuracy: 0.9706 - val_loss: 0.8048\n",
      "Epoch 10/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.9244 - loss: 0.8096 - val_accuracy: 1.0000 - val_loss: 0.7775\n",
      "Epoch 11/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.8894 - loss: 0.8105 - val_accuracy: 1.0000 - val_loss: 0.7546\n",
      "Epoch 12/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8937 - loss: 0.7868 - val_accuracy: 1.0000 - val_loss: 0.7351\n",
      "Epoch 13/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.8929 - loss: 0.7642 - val_accuracy: 1.0000 - val_loss: 0.7156\n",
      "Epoch 14/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.8920 - loss: 0.7428 - val_accuracy: 1.0000 - val_loss: 0.6952\n",
      "Epoch 15/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8605 - loss: 0.7321 - val_accuracy: 1.0000 - val_loss: 0.6770\n",
      "Epoch 16/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8786 - loss: 0.7073 - val_accuracy: 1.0000 - val_loss: 0.6588\n",
      "Epoch 17/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8995 - loss: 0.6955 - val_accuracy: 1.0000 - val_loss: 0.6421\n",
      "Epoch 18/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8686 - loss: 0.6821 - val_accuracy: 1.0000 - val_loss: 0.6275\n",
      "Epoch 19/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9014 - loss: 0.6643 - val_accuracy: 1.0000 - val_loss: 0.6127\n",
      "Epoch 20/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9085 - loss: 0.6421 - val_accuracy: 1.0000 - val_loss: 0.6006\n",
      "Epoch 21/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.8959 - loss: 0.6393 - val_accuracy: 1.0000 - val_loss: 0.5896\n",
      "Epoch 22/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9054 - loss: 0.6282 - val_accuracy: 0.9706 - val_loss: 0.5808\n",
      "Epoch 23/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9019 - loss: 0.6145 - val_accuracy: 0.9706 - val_loss: 0.5701\n",
      "Epoch 24/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9270 - loss: 0.5986 - val_accuracy: 0.9706 - val_loss: 0.5593\n",
      "Epoch 25/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8988 - loss: 0.5956 - val_accuracy: 1.0000 - val_loss: 0.5461\n",
      "Epoch 26/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9144 - loss: 0.5784 - val_accuracy: 1.0000 - val_loss: 0.5297\n",
      "Epoch 27/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8939 - loss: 0.5757 - val_accuracy: 1.0000 - val_loss: 0.5138\n",
      "Epoch 28/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9033 - loss: 0.5578 - val_accuracy: 1.0000 - val_loss: 0.4993\n",
      "Epoch 29/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9097 - loss: 0.5369 - val_accuracy: 1.0000 - val_loss: 0.4881\n",
      "Epoch 30/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9153 - loss: 0.5315 - val_accuracy: 1.0000 - val_loss: 0.4787\n",
      "Epoch 31/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9041 - loss: 0.5419 - val_accuracy: 1.0000 - val_loss: 0.4717\n",
      "Epoch 32/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.9102 - loss: 0.5103 - val_accuracy: 1.0000 - val_loss: 0.4631\n",
      "Epoch 33/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.9037 - loss: 0.4959 - val_accuracy: 1.0000 - val_loss: 0.4529\n",
      "Epoch 34/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8777 - loss: 0.5169 - val_accuracy: 1.0000 - val_loss: 0.4426\n",
      "Epoch 35/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8989 - loss: 0.5061 - val_accuracy: 1.0000 - val_loss: 0.4321\n",
      "Epoch 36/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.9041 - loss: 0.4966 - val_accuracy: 1.0000 - val_loss: 0.4226\n",
      "Epoch 37/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.8884 - loss: 0.4962 - val_accuracy: 1.0000 - val_loss: 0.4132\n",
      "Epoch 38/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8993 - loss: 0.4684 - val_accuracy: 1.0000 - val_loss: 0.4047\n",
      "Epoch 39/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.8747 - loss: 0.4955 - val_accuracy: 1.0000 - val_loss: 0.3978\n",
      "Epoch 40/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.9010 - loss: 0.4768 - val_accuracy: 1.0000 - val_loss: 0.3922\n",
      "Epoch 41/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8858 - loss: 0.4805 - val_accuracy: 1.0000 - val_loss: 0.3857\n",
      "Epoch 42/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8854 - loss: 0.4655 - val_accuracy: 1.0000 - val_loss: 0.3781\n",
      "Epoch 43/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.8858 - loss: 0.4619 - val_accuracy: 1.0000 - val_loss: 0.3706\n",
      "Epoch 44/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9146 - loss: 0.4212 - val_accuracy: 1.0000 - val_loss: 0.3644\n",
      "Epoch 45/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8810 - loss: 0.4358 - val_accuracy: 1.0000 - val_loss: 0.3605\n",
      "Epoch 46/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.9223 - loss: 0.4023 - val_accuracy: 1.0000 - val_loss: 0.3566\n",
      "Epoch 47/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8937 - loss: 0.4175 - val_accuracy: 1.0000 - val_loss: 0.3535\n",
      "Epoch 48/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8859 - loss: 0.4206 - val_accuracy: 1.0000 - val_loss: 0.3491\n",
      "Epoch 49/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9110 - loss: 0.4052 - val_accuracy: 1.0000 - val_loss: 0.3433\n",
      "Epoch 50/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9284 - loss: 0.3818 - val_accuracy: 1.0000 - val_loss: 0.3371\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000001DE713E8280> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/2\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 64ms/stepWARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000001DE713E8280> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n",
      "Subclassed Model Accuracy: 0.9285714285714286\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.90      0.86        10\n",
      "           1       1.00      1.00      1.00        15\n",
      "           2       0.94      0.88      0.91        17\n",
      "\n",
      "    accuracy                           0.93        42\n",
      "   macro avg       0.92      0.93      0.92        42\n",
      "weighted avg       0.93      0.93      0.93        42\n",
      "\n",
      "[[ 9  0  1]\n",
      " [ 0 15  0]\n",
      " [ 2  0 15]]\n"
     ]
    }
   ],
   "source": [
    "# Model 3: Model 클래스 상속\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "class SimpleMLP(Model):\n",
    "    def __init__(self, num_classes):\n",
    "        super(SimpleMLP, self).__init__()\n",
    "        self.dense1 = Dense(32, activation='sigmoid')\n",
    "        self.dense2 = Dense(num_classes, activation='softmax')\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        x = self.dense1(inputs)\n",
    "        return self.dense2(x)\n",
    "\n",
    "model3 = SimpleMLP(num_classes=3)\n",
    "model3.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model3.fit(X_train, y_train, epochs=50, validation_split=0.2)\n",
    "\n",
    "# 예측 및 평가\n",
    "y_pred_prob = model3.predict(X_test)\n",
    "y_pred_class = np.argmax(y_pred_prob, axis=1)\n",
    "y_test_class = np.argmax(y_test, axis=1)\n",
    "\n",
    "print(\"Subclassed Model Accuracy:\", np.mean(y_test_class == y_pred_class))\n",
    "print(classification_report(y_test_class, y_pred_class))\n",
    "print(confusion_matrix(y_test_class, y_pred_class))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
