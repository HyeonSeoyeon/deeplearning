{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4330632e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4271c2a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>15.26</th>\n",
       "      <th>14.84</th>\n",
       "      <th>0.871</th>\n",
       "      <th>5.763</th>\n",
       "      <th>3.312</th>\n",
       "      <th>2.221</th>\n",
       "      <th>5.22</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.88</td>\n",
       "      <td>14.57</td>\n",
       "      <td>0.8811</td>\n",
       "      <td>5.554</td>\n",
       "      <td>3.333</td>\n",
       "      <td>1.018</td>\n",
       "      <td>4.956</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14.29</td>\n",
       "      <td>14.09</td>\n",
       "      <td>0.9050</td>\n",
       "      <td>5.291</td>\n",
       "      <td>3.337</td>\n",
       "      <td>2.699</td>\n",
       "      <td>4.825</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.84</td>\n",
       "      <td>13.94</td>\n",
       "      <td>0.8955</td>\n",
       "      <td>5.324</td>\n",
       "      <td>3.379</td>\n",
       "      <td>2.259</td>\n",
       "      <td>4.805</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.14</td>\n",
       "      <td>14.99</td>\n",
       "      <td>0.9034</td>\n",
       "      <td>5.658</td>\n",
       "      <td>3.562</td>\n",
       "      <td>1.355</td>\n",
       "      <td>5.175</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14.38</td>\n",
       "      <td>14.21</td>\n",
       "      <td>0.8951</td>\n",
       "      <td>5.386</td>\n",
       "      <td>3.312</td>\n",
       "      <td>2.462</td>\n",
       "      <td>4.956</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>12.19</td>\n",
       "      <td>13.20</td>\n",
       "      <td>0.8783</td>\n",
       "      <td>5.137</td>\n",
       "      <td>2.981</td>\n",
       "      <td>3.631</td>\n",
       "      <td>4.870</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>11.23</td>\n",
       "      <td>12.88</td>\n",
       "      <td>0.8511</td>\n",
       "      <td>5.140</td>\n",
       "      <td>2.795</td>\n",
       "      <td>4.325</td>\n",
       "      <td>5.003</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>13.20</td>\n",
       "      <td>13.66</td>\n",
       "      <td>0.8883</td>\n",
       "      <td>5.236</td>\n",
       "      <td>3.232</td>\n",
       "      <td>8.315</td>\n",
       "      <td>5.056</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>11.84</td>\n",
       "      <td>13.21</td>\n",
       "      <td>0.8521</td>\n",
       "      <td>5.175</td>\n",
       "      <td>2.836</td>\n",
       "      <td>3.598</td>\n",
       "      <td>5.044</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>12.30</td>\n",
       "      <td>13.34</td>\n",
       "      <td>0.8684</td>\n",
       "      <td>5.243</td>\n",
       "      <td>2.974</td>\n",
       "      <td>5.637</td>\n",
       "      <td>5.063</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>209 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     15.26  14.84   0.871  5.763  3.312  2.221   5.22  1\n",
       "0    14.88  14.57  0.8811  5.554  3.333  1.018  4.956  1\n",
       "1    14.29  14.09  0.9050  5.291  3.337  2.699  4.825  1\n",
       "2    13.84  13.94  0.8955  5.324  3.379  2.259  4.805  1\n",
       "3    16.14  14.99  0.9034  5.658  3.562  1.355  5.175  1\n",
       "4    14.38  14.21  0.8951  5.386  3.312  2.462  4.956  1\n",
       "..     ...    ...     ...    ...    ...    ...    ... ..\n",
       "204  12.19  13.20  0.8783  5.137  2.981  3.631  4.870  3\n",
       "205  11.23  12.88  0.8511  5.140  2.795  4.325  5.003  3\n",
       "206  13.20  13.66  0.8883  5.236  3.232  8.315  5.056  3\n",
       "207  11.84  13.21  0.8521  5.175  2.836  3.598  5.044  3\n",
       "208  12.30  13.34  0.8684  5.243  2.974  5.637  5.063  3\n",
       "\n",
       "[209 rows x 8 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 로드\n",
    "df = pd.read_csv(\"D:/seeds.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "54156b33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "결측치 확인:\n",
      "15.26    0\n",
      "14.84    0\n",
      "0.871    0\n",
      "5.763    0\n",
      "3.312    0\n",
      "2.221    0\n",
      "5.22     0\n",
      "1        0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 결측치 확인\n",
    "print(\"\\n결측치 확인:\")\n",
    "print(df.isnull().sum()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "39d756cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "imbalanced data 확인:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 레이블 개수 확인(imbalanced data) - 그래프\n",
    "print(\"\\nimbalanced data 확인:\")\n",
    "plt.figure(figsize=(6, 4))  # 그래프 크기 설정\n",
    "\n",
    "sns.countplot(data = df, x=\"1\")\n",
    "plt.xlabel(\"1\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"target\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8b53686c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[14.88  , 14.57  ,  0.8811, ...,  3.333 ,  1.018 ,  4.956 ],\n",
       "       [14.29  , 14.09  ,  0.905 , ...,  3.337 ,  2.699 ,  4.825 ],\n",
       "       [13.84  , 13.94  ,  0.8955, ...,  3.379 ,  2.259 ,  4.805 ],\n",
       "       ...,\n",
       "       [13.2   , 13.66  ,  0.8883, ...,  3.232 ,  8.315 ,  5.056 ],\n",
       "       [11.84  , 13.21  ,  0.8521, ...,  2.836 ,  3.598 ,  5.044 ],\n",
       "       [12.3   , 13.34  ,  0.8684, ...,  2.974 ,  5.637 ,  5.063 ]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 입력값과 타겟 분리\n",
    "X = df.iloc[:, :-1].values\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0e3a95eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], dtype=int64)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 입력값과 타겟 분리\n",
    "y = df.iloc[:, -1].values -1  # 레이블 0,1,2로 변환\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6f502390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "레이블 개수 확인:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2    70\n",
       "3    70\n",
       "1    69\n",
       "Name: 1, dtype: int64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 레이블 확인 - 숫자\n",
    "print(\"\\n레이블 개수 확인:\")\n",
    "df['1'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a63e033a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.0118402 ,  0.00923364,  0.42749407, ...,  0.19764747,\n",
       "        -1.79278662, -0.9219711 ],\n",
       "       [-0.19093968, -0.35835326,  1.43894519, ...,  0.20823799,\n",
       "        -0.67216102, -1.18860657],\n",
       "       [-0.3456023 , -0.47322416,  1.03690395, ...,  0.31943844,\n",
       "        -0.9654836 , -1.22931428],\n",
       "       ...,\n",
       "       [-0.56556692, -0.68764985,  0.73219901, ..., -0.06976315,\n",
       "         3.07170181, -0.71843257],\n",
       "       [-1.03299173, -1.03226257, -0.79978973, ..., -1.11822457,\n",
       "        -0.07284964, -0.7428572 ],\n",
       "       [-0.87489216, -0.93270779, -0.10997159, ..., -0.75285165,\n",
       "         1.28643389, -0.70418488]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 정규화\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9ce6340c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 원-핫 인코딩\n",
    "y = to_categorical(y)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6271c77e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((167, 7), (42, 7), (167, 3), (42, 3))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 학습/테스트 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9812cde7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tjdus\\anaconda3\\lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 117ms/step - accuracy: 0.3697 - loss: 1.2227 - val_accuracy: 0.2647 - val_loss: 1.3110\n",
      "Epoch 2/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.3701 - loss: 1.1565 - val_accuracy: 0.2647 - val_loss: 1.2281\n",
      "Epoch 3/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.3818 - loss: 1.0927 - val_accuracy: 0.2647 - val_loss: 1.1523\n",
      "Epoch 4/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.3571 - loss: 1.0488 - val_accuracy: 0.2647 - val_loss: 1.0859\n",
      "Epoch 5/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.3942 - loss: 0.9910 - val_accuracy: 0.2941 - val_loss: 1.0265\n",
      "Epoch 6/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.4513 - loss: 0.9408 - val_accuracy: 0.3235 - val_loss: 0.9736\n",
      "Epoch 7/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.4703 - loss: 0.9147 - val_accuracy: 0.4706 - val_loss: 0.9280\n",
      "Epoch 8/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.5771 - loss: 0.8730 - val_accuracy: 0.5882 - val_loss: 0.8891\n",
      "Epoch 9/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.6554 - loss: 0.8528 - val_accuracy: 0.7059 - val_loss: 0.8563\n",
      "Epoch 10/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.7463 - loss: 0.8330 - val_accuracy: 0.8529 - val_loss: 0.8259\n",
      "Epoch 11/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.8572 - loss: 0.8082 - val_accuracy: 0.8824 - val_loss: 0.7982\n",
      "Epoch 12/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.9122 - loss: 0.8013 - val_accuracy: 0.9118 - val_loss: 0.7740\n",
      "Epoch 13/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.9056 - loss: 0.7819 - val_accuracy: 0.9412 - val_loss: 0.7523\n",
      "Epoch 14/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 0.9130 - loss: 0.7640 - val_accuracy: 1.0000 - val_loss: 0.7318\n",
      "Epoch 15/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.8941 - loss: 0.7519 - val_accuracy: 1.0000 - val_loss: 0.7116\n",
      "Epoch 16/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.9010 - loss: 0.7367 - val_accuracy: 1.0000 - val_loss: 0.6915\n",
      "Epoch 17/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.9012 - loss: 0.7024 - val_accuracy: 1.0000 - val_loss: 0.6717\n",
      "Epoch 18/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.9069 - loss: 0.6855 - val_accuracy: 1.0000 - val_loss: 0.6551\n",
      "Epoch 19/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.8887 - loss: 0.6848 - val_accuracy: 1.0000 - val_loss: 0.6403\n",
      "Epoch 20/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.8774 - loss: 0.6737 - val_accuracy: 1.0000 - val_loss: 0.6269\n",
      "Epoch 21/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.8770 - loss: 0.6610 - val_accuracy: 1.0000 - val_loss: 0.6141\n",
      "Epoch 22/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.8757 - loss: 0.6508 - val_accuracy: 1.0000 - val_loss: 0.6013\n",
      "Epoch 23/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.8826 - loss: 0.6383 - val_accuracy: 1.0000 - val_loss: 0.5889\n",
      "Epoch 24/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9026 - loss: 0.6153 - val_accuracy: 1.0000 - val_loss: 0.5767\n",
      "Epoch 25/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.8796 - loss: 0.6145 - val_accuracy: 1.0000 - val_loss: 0.5658\n",
      "Epoch 26/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.8991 - loss: 0.5946 - val_accuracy: 1.0000 - val_loss: 0.5547\n",
      "Epoch 27/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.8704 - loss: 0.5978 - val_accuracy: 1.0000 - val_loss: 0.5457\n",
      "Epoch 28/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.8747 - loss: 0.5855 - val_accuracy: 1.0000 - val_loss: 0.5360\n",
      "Epoch 29/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8898 - loss: 0.5719 - val_accuracy: 1.0000 - val_loss: 0.5265\n",
      "Epoch 30/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.8842 - loss: 0.5651 - val_accuracy: 1.0000 - val_loss: 0.5161\n",
      "Epoch 31/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.8963 - loss: 0.5460 - val_accuracy: 1.0000 - val_loss: 0.5069\n",
      "Epoch 32/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.8975 - loss: 0.5512 - val_accuracy: 1.0000 - val_loss: 0.5005\n",
      "Epoch 33/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9148 - loss: 0.5346 - val_accuracy: 1.0000 - val_loss: 0.4925\n",
      "Epoch 34/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.9118 - loss: 0.5257 - val_accuracy: 1.0000 - val_loss: 0.4840\n",
      "Epoch 35/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9036 - loss: 0.5307 - val_accuracy: 1.0000 - val_loss: 0.4745\n",
      "Epoch 36/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8932 - loss: 0.5156 - val_accuracy: 1.0000 - val_loss: 0.4644\n",
      "Epoch 37/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.8846 - loss: 0.4980 - val_accuracy: 1.0000 - val_loss: 0.4558\n",
      "Epoch 38/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 0.9141 - loss: 0.4835 - val_accuracy: 1.0000 - val_loss: 0.4461\n",
      "Epoch 39/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.8941 - loss: 0.4934 - val_accuracy: 1.0000 - val_loss: 0.4392\n",
      "Epoch 40/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9184 - loss: 0.4672 - val_accuracy: 1.0000 - val_loss: 0.4327\n",
      "Epoch 41/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9068 - loss: 0.4613 - val_accuracy: 1.0000 - val_loss: 0.4268\n",
      "Epoch 42/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9114 - loss: 0.4745 - val_accuracy: 1.0000 - val_loss: 0.4203\n",
      "Epoch 43/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9045 - loss: 0.4707 - val_accuracy: 1.0000 - val_loss: 0.4133\n",
      "Epoch 44/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8993 - loss: 0.4696 - val_accuracy: 1.0000 - val_loss: 0.4083\n",
      "Epoch 45/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.8996 - loss: 0.4549 - val_accuracy: 1.0000 - val_loss: 0.4030\n",
      "Epoch 46/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.8970 - loss: 0.4456 - val_accuracy: 1.0000 - val_loss: 0.3967\n",
      "Epoch 47/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.9187 - loss: 0.4314 - val_accuracy: 1.0000 - val_loss: 0.3895\n",
      "Epoch 48/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.9093 - loss: 0.4231 - val_accuracy: 1.0000 - val_loss: 0.3813\n",
      "Epoch 49/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9323 - loss: 0.4146 - val_accuracy: 1.0000 - val_loss: 0.3746\n",
      "Epoch 50/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8945 - loss: 0.4363 - val_accuracy: 1.0000 - val_loss: 0.3674\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step \n",
      "Sequential Model Accuracy: 0.9285714285714286\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.90      0.86        10\n",
      "           1       1.00      1.00      1.00        15\n",
      "           2       0.94      0.88      0.91        17\n",
      "\n",
      "    accuracy                           0.93        42\n",
      "   macro avg       0.92      0.93      0.92        42\n",
      "weighted avg       0.93      0.93      0.93        42\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 9  0  1]\n",
      " [ 0 15  0]\n",
      " [ 2  0 15]]\n"
     ]
    }
   ],
   "source": [
    "# Model 1: Sequential 모델 (add()로 레이어 추가)\n",
    "\n",
    "# Sequential 모델 정의\n",
    "model1 = Sequential()\n",
    "model1.add(Dense(units=32, input_shape=(X.shape[1],), activation='sigmoid'))\n",
    "model1.add(Dense(units=3, activation='softmax'))\n",
    "\n",
    "model1.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model1.fit(X_train, y_train, epochs=50, validation_split=0.2)\n",
    "\n",
    "# 예측 및 평가\n",
    "y_pred_prob = model1.predict(X_test)\n",
    "y_pred_class = np.argmax(y_pred_prob, axis=1)\n",
    "y_test_class = np.argmax(y_test, axis=1)\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(\"Sequential Model Accuracy:\", np.mean(y_test_class == y_pred_class))\n",
    "print(classification_report(y_test_class, y_pred_class))\n",
    "print(confusion_matrix(y_test_class, y_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e1988a46",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.3158 - loss: 1.2855 - val_accuracy: 0.3824 - val_loss: 1.2388\n",
      "Epoch 2/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.3492 - loss: 1.1995 - val_accuracy: 0.3824 - val_loss: 1.1797\n",
      "Epoch 3/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.3313 - loss: 1.1428 - val_accuracy: 0.3824 - val_loss: 1.1303\n",
      "Epoch 4/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.3865 - loss: 1.1006 - val_accuracy: 0.4412 - val_loss: 1.0843\n",
      "Epoch 5/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.5166 - loss: 1.0269 - val_accuracy: 0.5294 - val_loss: 1.0411\n",
      "Epoch 6/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.5646 - loss: 1.0146 - val_accuracy: 0.5882 - val_loss: 0.9996\n",
      "Epoch 7/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.6460 - loss: 0.9826 - val_accuracy: 0.6176 - val_loss: 0.9639\n",
      "Epoch 8/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.6508 - loss: 0.9301 - val_accuracy: 0.6471 - val_loss: 0.9344\n",
      "Epoch 9/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.6478 - loss: 0.9185 - val_accuracy: 0.6471 - val_loss: 0.9042\n",
      "Epoch 10/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.6047 - loss: 0.9182 - val_accuracy: 0.6471 - val_loss: 0.8737\n",
      "Epoch 11/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.6273 - loss: 0.8703 - val_accuracy: 0.6471 - val_loss: 0.8476\n",
      "Epoch 12/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.6341 - loss: 0.8613 - val_accuracy: 0.7059 - val_loss: 0.8248\n",
      "Epoch 13/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.6549 - loss: 0.8297 - val_accuracy: 0.7353 - val_loss: 0.8010\n",
      "Epoch 14/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.7281 - loss: 0.8205 - val_accuracy: 0.8824 - val_loss: 0.7785\n",
      "Epoch 15/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.8290 - loss: 0.8024 - val_accuracy: 0.9706 - val_loss: 0.7560\n",
      "Epoch 16/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9093 - loss: 0.7813 - val_accuracy: 0.9706 - val_loss: 0.7360\n",
      "Epoch 17/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9049 - loss: 0.7706 - val_accuracy: 1.0000 - val_loss: 0.7186\n",
      "Epoch 18/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.8804 - loss: 0.7479 - val_accuracy: 0.9706 - val_loss: 0.7004\n",
      "Epoch 19/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.8847 - loss: 0.7434 - val_accuracy: 0.9706 - val_loss: 0.6836\n",
      "Epoch 20/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 0.8825 - loss: 0.7145 - val_accuracy: 0.9706 - val_loss: 0.6685\n",
      "Epoch 21/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.8960 - loss: 0.7241 - val_accuracy: 0.9706 - val_loss: 0.6550\n",
      "Epoch 22/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8808 - loss: 0.7037 - val_accuracy: 0.9706 - val_loss: 0.6424\n",
      "Epoch 23/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8757 - loss: 0.6834 - val_accuracy: 1.0000 - val_loss: 0.6319\n",
      "Epoch 24/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.9041 - loss: 0.6600 - val_accuracy: 1.0000 - val_loss: 0.6199\n",
      "Epoch 25/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 0.9087 - loss: 0.6525 - val_accuracy: 1.0000 - val_loss: 0.6103\n",
      "Epoch 26/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9166 - loss: 0.6474 - val_accuracy: 1.0000 - val_loss: 0.6011\n",
      "Epoch 27/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9152 - loss: 0.6340 - val_accuracy: 0.9706 - val_loss: 0.5928\n",
      "Epoch 28/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9205 - loss: 0.6198 - val_accuracy: 0.9706 - val_loss: 0.5844\n",
      "Epoch 29/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9123 - loss: 0.6012 - val_accuracy: 0.9706 - val_loss: 0.5719\n",
      "Epoch 30/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8984 - loss: 0.6000 - val_accuracy: 1.0000 - val_loss: 0.5589\n",
      "Epoch 31/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9010 - loss: 0.5980 - val_accuracy: 1.0000 - val_loss: 0.5458\n",
      "Epoch 32/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9010 - loss: 0.5873 - val_accuracy: 1.0000 - val_loss: 0.5341\n",
      "Epoch 33/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9067 - loss: 0.5758 - val_accuracy: 1.0000 - val_loss: 0.5243\n",
      "Epoch 34/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.9106 - loss: 0.5540 - val_accuracy: 1.0000 - val_loss: 0.5165\n",
      "Epoch 35/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.9161 - loss: 0.5615 - val_accuracy: 1.0000 - val_loss: 0.5087\n",
      "Epoch 36/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9154 - loss: 0.5315 - val_accuracy: 1.0000 - val_loss: 0.4996\n",
      "Epoch 37/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9075 - loss: 0.5361 - val_accuracy: 1.0000 - val_loss: 0.4877\n",
      "Epoch 38/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9174 - loss: 0.5250 - val_accuracy: 1.0000 - val_loss: 0.4771\n",
      "Epoch 39/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8998 - loss: 0.5070 - val_accuracy: 0.9706 - val_loss: 0.4657\n",
      "Epoch 40/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9110 - loss: 0.5144 - val_accuracy: 0.9706 - val_loss: 0.4557\n",
      "Epoch 41/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9010 - loss: 0.4972 - val_accuracy: 0.9706 - val_loss: 0.4491\n",
      "Epoch 42/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9001 - loss: 0.4947 - val_accuracy: 0.9706 - val_loss: 0.4425\n",
      "Epoch 43/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8975 - loss: 0.4920 - val_accuracy: 1.0000 - val_loss: 0.4363\n",
      "Epoch 44/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9063 - loss: 0.4860 - val_accuracy: 1.0000 - val_loss: 0.4309\n",
      "Epoch 45/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9127 - loss: 0.4731 - val_accuracy: 1.0000 - val_loss: 0.4251\n",
      "Epoch 46/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.8850 - loss: 0.4921 - val_accuracy: 1.0000 - val_loss: 0.4199\n",
      "Epoch 47/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9171 - loss: 0.4532 - val_accuracy: 1.0000 - val_loss: 0.4159\n",
      "Epoch 48/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8932 - loss: 0.4682 - val_accuracy: 1.0000 - val_loss: 0.4097\n",
      "Epoch 49/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.9040 - loss: 0.4673 - val_accuracy: 1.0000 - val_loss: 0.4031\n",
      "Epoch 50/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.8984 - loss: 0.4559 - val_accuracy: 1.0000 - val_loss: 0.3948\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n",
      "Functional API Model Accuracy: 0.9285714285714286\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.90      0.86        10\n",
      "           1       1.00      1.00      1.00        15\n",
      "           2       0.94      0.88      0.91        17\n",
      "\n",
      "    accuracy                           0.93        42\n",
      "   macro avg       0.92      0.93      0.92        42\n",
      "weighted avg       0.93      0.93      0.93        42\n",
      "\n",
      "[[ 9  0  1]\n",
      " [ 0 15  0]\n",
      " [ 2  0 15]]\n"
     ]
    }
   ],
   "source": [
    "# Model 2: 함수형 API\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "\n",
    "# Functional API 모델 정의\n",
    "inputs = Input(shape=(X.shape[1],))\n",
    "x = Dense(32, activation='sigmoid')(inputs)\n",
    "outputs = Dense(3, activation='softmax')(x)\n",
    "model2 = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "model2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model2.fit(X_train, y_train, epochs=50, validation_split=0.2)\n",
    "\n",
    "# 예측 및 평가\n",
    "y_pred_prob = model2.predict(X_test)\n",
    "y_pred_class = np.argmax(y_pred_prob, axis=1)\n",
    "y_test_class = np.argmax(y_test, axis=1)\n",
    "\n",
    "print(\"Functional API Model Accuracy:\", np.mean(y_test_class == y_pred_class))\n",
    "print(classification_report(y_test_class, y_pred_class))\n",
    "print(confusion_matrix(y_test_class, y_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6153be3c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.3310 - loss: 1.0710 - val_accuracy: 0.3235 - val_loss: 1.0512\n",
      "Epoch 2/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.4572 - loss: 1.0178 - val_accuracy: 0.4706 - val_loss: 1.0144\n",
      "Epoch 3/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.5102 - loss: 0.9972 - val_accuracy: 0.5000 - val_loss: 0.9780\n",
      "Epoch 4/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.6585 - loss: 0.9554 - val_accuracy: 0.5882 - val_loss: 0.9480\n",
      "Epoch 5/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.6875 - loss: 0.9316 - val_accuracy: 0.6765 - val_loss: 0.9176\n",
      "Epoch 6/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.7578 - loss: 0.9050 - val_accuracy: 0.7647 - val_loss: 0.8888\n",
      "Epoch 7/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.7849 - loss: 0.8884 - val_accuracy: 0.8824 - val_loss: 0.8595\n",
      "Epoch 8/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.8408 - loss: 0.8606 - val_accuracy: 0.9706 - val_loss: 0.8329\n",
      "Epoch 9/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.8718 - loss: 0.8396 - val_accuracy: 0.9706 - val_loss: 0.8048\n",
      "Epoch 10/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.9244 - loss: 0.8096 - val_accuracy: 1.0000 - val_loss: 0.7775\n",
      "Epoch 11/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.8894 - loss: 0.8105 - val_accuracy: 1.0000 - val_loss: 0.7546\n",
      "Epoch 12/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8937 - loss: 0.7868 - val_accuracy: 1.0000 - val_loss: 0.7351\n",
      "Epoch 13/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.8929 - loss: 0.7642 - val_accuracy: 1.0000 - val_loss: 0.7156\n",
      "Epoch 14/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.8920 - loss: 0.7428 - val_accuracy: 1.0000 - val_loss: 0.6952\n",
      "Epoch 15/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8605 - loss: 0.7321 - val_accuracy: 1.0000 - val_loss: 0.6770\n",
      "Epoch 16/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8786 - loss: 0.7073 - val_accuracy: 1.0000 - val_loss: 0.6588\n",
      "Epoch 17/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8995 - loss: 0.6955 - val_accuracy: 1.0000 - val_loss: 0.6421\n",
      "Epoch 18/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8686 - loss: 0.6821 - val_accuracy: 1.0000 - val_loss: 0.6275\n",
      "Epoch 19/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9014 - loss: 0.6643 - val_accuracy: 1.0000 - val_loss: 0.6127\n",
      "Epoch 20/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9085 - loss: 0.6421 - val_accuracy: 1.0000 - val_loss: 0.6006\n",
      "Epoch 21/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.8959 - loss: 0.6393 - val_accuracy: 1.0000 - val_loss: 0.5896\n",
      "Epoch 22/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9054 - loss: 0.6282 - val_accuracy: 0.9706 - val_loss: 0.5808\n",
      "Epoch 23/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9019 - loss: 0.6145 - val_accuracy: 0.9706 - val_loss: 0.5701\n",
      "Epoch 24/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9270 - loss: 0.5986 - val_accuracy: 0.9706 - val_loss: 0.5593\n",
      "Epoch 25/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8988 - loss: 0.5956 - val_accuracy: 1.0000 - val_loss: 0.5461\n",
      "Epoch 26/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9144 - loss: 0.5784 - val_accuracy: 1.0000 - val_loss: 0.5297\n",
      "Epoch 27/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8939 - loss: 0.5757 - val_accuracy: 1.0000 - val_loss: 0.5138\n",
      "Epoch 28/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9033 - loss: 0.5578 - val_accuracy: 1.0000 - val_loss: 0.4993\n",
      "Epoch 29/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9097 - loss: 0.5369 - val_accuracy: 1.0000 - val_loss: 0.4881\n",
      "Epoch 30/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9153 - loss: 0.5315 - val_accuracy: 1.0000 - val_loss: 0.4787\n",
      "Epoch 31/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9041 - loss: 0.5419 - val_accuracy: 1.0000 - val_loss: 0.4717\n",
      "Epoch 32/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.9102 - loss: 0.5103 - val_accuracy: 1.0000 - val_loss: 0.4631\n",
      "Epoch 33/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.9037 - loss: 0.4959 - val_accuracy: 1.0000 - val_loss: 0.4529\n",
      "Epoch 34/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8777 - loss: 0.5169 - val_accuracy: 1.0000 - val_loss: 0.4426\n",
      "Epoch 35/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8989 - loss: 0.5061 - val_accuracy: 1.0000 - val_loss: 0.4321\n",
      "Epoch 36/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.9041 - loss: 0.4966 - val_accuracy: 1.0000 - val_loss: 0.4226\n",
      "Epoch 37/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.8884 - loss: 0.4962 - val_accuracy: 1.0000 - val_loss: 0.4132\n",
      "Epoch 38/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8993 - loss: 0.4684 - val_accuracy: 1.0000 - val_loss: 0.4047\n",
      "Epoch 39/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.8747 - loss: 0.4955 - val_accuracy: 1.0000 - val_loss: 0.3978\n",
      "Epoch 40/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.9010 - loss: 0.4768 - val_accuracy: 1.0000 - val_loss: 0.3922\n",
      "Epoch 41/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8858 - loss: 0.4805 - val_accuracy: 1.0000 - val_loss: 0.3857\n",
      "Epoch 42/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8854 - loss: 0.4655 - val_accuracy: 1.0000 - val_loss: 0.3781\n",
      "Epoch 43/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.8858 - loss: 0.4619 - val_accuracy: 1.0000 - val_loss: 0.3706\n",
      "Epoch 44/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9146 - loss: 0.4212 - val_accuracy: 1.0000 - val_loss: 0.3644\n",
      "Epoch 45/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8810 - loss: 0.4358 - val_accuracy: 1.0000 - val_loss: 0.3605\n",
      "Epoch 46/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.9223 - loss: 0.4023 - val_accuracy: 1.0000 - val_loss: 0.3566\n",
      "Epoch 47/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8937 - loss: 0.4175 - val_accuracy: 1.0000 - val_loss: 0.3535\n",
      "Epoch 48/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8859 - loss: 0.4206 - val_accuracy: 1.0000 - val_loss: 0.3491\n",
      "Epoch 49/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9110 - loss: 0.4052 - val_accuracy: 1.0000 - val_loss: 0.3433\n",
      "Epoch 50/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9284 - loss: 0.3818 - val_accuracy: 1.0000 - val_loss: 0.3371\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000001DE713E8280> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/2\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 64ms/stepWARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000001DE713E8280> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n",
      "Subclassed Model Accuracy: 0.9285714285714286\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.90      0.86        10\n",
      "           1       1.00      1.00      1.00        15\n",
      "           2       0.94      0.88      0.91        17\n",
      "\n",
      "    accuracy                           0.93        42\n",
      "   macro avg       0.92      0.93      0.92        42\n",
      "weighted avg       0.93      0.93      0.93        42\n",
      "\n",
      "[[ 9  0  1]\n",
      " [ 0 15  0]\n",
      " [ 2  0 15]]\n"
     ]
    }
   ],
   "source": [
    "# Model 3: Model 클래스 상속\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "class SimpleMLP(Model):\n",
    "    def __init__(self, num_classes):\n",
    "        super(SimpleMLP, self).__init__()\n",
    "        self.dense1 = Dense(32, activation='sigmoid')\n",
    "        self.dense2 = Dense(num_classes, activation='softmax')\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        x = self.dense1(inputs)\n",
    "        return self.dense2(x)\n",
    "\n",
    "model3 = SimpleMLP(num_classes=3)\n",
    "model3.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model3.fit(X_train, y_train, epochs=50, validation_split=0.2)\n",
    "\n",
    "# 예측 및 평가\n",
    "y_pred_prob = model3.predict(X_test)\n",
    "y_pred_class = np.argmax(y_pred_prob, axis=1)\n",
    "y_test_class = np.argmax(y_test, axis=1)\n",
    "\n",
    "print(\"Subclassed Model Accuracy:\", np.mean(y_test_class == y_pred_class))\n",
    "print(classification_report(y_test_class, y_pred_class))\n",
    "print(confusion_matrix(y_test_class, y_pred_class))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
